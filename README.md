# kei08087-projects

## 프로젝트 1: Audio-record

### 문제 정의:

평소 언어 학습에 많은 관심이 있었으나, 시중 언어 학습 프로그램들은 단어 암기/듣기/쓰기 등에는 효과적이더라도, 말하기에서는 발음이 옳은지, 실제로는 어떻게 들리는지 잘 확인할 수 없는 경우가 많다. 그래서 실제로는 각 발음이 정확한지, 어떻게 인식되는지 확인하기 위해, 사람의 말을 녹음하여 언어를 추론, 문자로 변환해주는 프로그램을 구상 하게 되었다.

### 기술 스택:

**Flask:** 가볍게 웹의 백엔드 부분을 구축할 수 있는 프레임워크로, python 환경에서 작동한다. 
**선택 이유:** Python 환경에서 작동하는 다른 주요 웹 백엔드 프레임워크로는 Django나 FastAPI가 있는데, Flask가 풀스택 프레임워크인 Django에 비해 상대적으로 가볍다. 다만 FastAPI의 경우 추후 확장에 있어 고려 해 볼 만하다.

**Flask-cors:** 서로 다른 도메인에서 웹에 접근하는 것은 원칙적으로 거부되지만, 프론트 엔드와 백엔드가 서로 다른 도메인에서 동작하는 경우 이를 우회해야 한다. 이때 사용되는 것이 cors로, flask-cors는 이 기능을 flask에서 제공해 준다.
**선택 이유:** Flask-sock을 통한 과정에서 발생 가능한 도메인 충돌 상황을 해결하기 위해 사용 했다.

**Flask-sock:** Flask에서 webSocket을 통한 데이터 송수신을 가능하게 하는 프레임워크로, C 등에서 네트워크/전송 계층에 일일히 socket을 구현할 필요 없이 간편하게 연결을 구축할 수 있게 해준다.
**선택 이유:** Flask 환경 하에서, 손쉽게 웹 통신을 구현하기 위해 도입 되었다.

**Openai-whisper:** Openai 측에서 제공하는 음성 인식 모델로, 이 프로젝트의 코어 서비스를 담당한다.
**선택 이유:** 쉽게 음성 인식 모델을 구축할 수 있으며, 빠른 응답이 가능 하기에 선택되었다.

**Pyngrok:** Ngrok를 통한 로컬 서버 접근을 python 환경에서 가능하게 하는 프레임워크.
**선택 이유:** 구성 상 로컬 서버를 통해 웹이 생성되는데, 이때 접속을 용이하게 하기 위해 도입되었다.

**Soundfile:** Python에서 오디오 관련 입출력을 제공하는 라이브러리.
**선택 이유:** Python 환경에서 음성 처리를 수월하게 하기 위해 선택되었다.

**React:** 프론트엔드 라이브러리로, html과 css 문서들로 구축된 웹 페이지의 입출력을 조절하고, 백엔드의 flask와 상호작용 하는 역할이다.

위 기술 스택들 중, 이후 프로젝트에서 반복되는 기술 스택의 경우 중복 기술 하지 않을 것이다.

### 처리 메커니즘:

**1.** 웹에서 입력 받은 녹음 데이터를 app.js의 handleStartRecording에서 MediaRecorder을 통해 인식한 다음 배열의 형태로 저장하고, sendAudioData에서 해당 배열을 Blob으로 묶은 후 socket을 통해 app.py로 전달한다.

**2.** app.py는 해당 Blob을 전달 받아 wav 형식으로 저장한 후, openai-whisper에게 model.transcribe()로 전달한다

**3.** openai-whisper가 반환한 값을 텍스트의 형태로 다시 app.js에 전달한다

**4.** app.js의 socketRef.current.onmessage 부분이 텍스트를 수신하고, transcriptions에 저장한다

**5.** 저장된 내용을 React.createElement에서 검색한 다음, chat-bubble의 형태로 웹에 띄운다.

### 알고리즘:

기본적으로 트랜스포머가 적용된 openai-whisper를 사용한다. 트랜스포머는 자기 자신을 마스킹 해 feedback 하는 형식의 딥러닝 알고리즘으로, LLM 분야에서 현재까지 가장 효과적인 결과물을 내놓고 있다. 현실적으로, 현재 상황에서 언어와 관련된 인공지능 서비스를 원한다면 트랜스포머의 사용을 필연적이다.

### 개발 과정:

최초 개발 당시 webSocket url 추출을 자동이 아닌 sed 명령어를 통해 수동으로 변경하는 방식으로 진행 한 결과, 과정이 지나치게 번거로웠고, 또 자기 자신의 환경을 제대로 숙지 하지 못한 경우 정확한 url을 확보하지 못해 이후의 로직이 제대로 작동하지 않았다.
Ex) wss://로 고정 기입 했으나, 확인 결과 로컬 디버그 환경에서는 ws://로 접속해야 했음
그래서 url을 추출할 때, protocol까지 추적해 현재 환경에 맞는 url을 반환하도록 구조를 변경해야 했다.

### 습득한 기술:

**Flask와 보조 프레임워크의 활용:** Flask를 통해 웹의 백엔드를 구축하고, Cors 문제 해결 및 Socket 연결 등을 익힐 수 있었다.

**Openai-whisper와 SoundFile, MediaRecorder 등의 기능 활용:** 다소 도메인에 특화되어 있는 기능이긴 하지만, 해당 기술들을 실전에서 사용해 보며 사용법을 익힐 수 있었다.

**React 라이브러리 활용:** React를 포함한 프론트엔드 시스템을 구축하고, html/css와 연결시키는 경험을 통해 프론트엔드와 관련된 이해를 높을 수 있었다.

### 결론 및 개선 사항:

이 서비스를 테스트하는 과정에서 한국어, 영어, 중국어, 일본어의 경우 손쉽게 문자로 변환해 내는 모습을 보였지만, 나머지 언어에 대해서는 다소 문자열 변환 성능이 아쉬웠다. 이 문제를 해결 하는 것이 서비스의 보편성 향상을 위해 중요할 것으로 보인다.
또, 만약 이 서비스를 현재의 선 녹음 후 변환 구조에서 탈피해, 실시간 변환으로 확장하고 싶다면 기본 틀을 Flask에서 FastAPI로 전환 하는 것도 고려 해 볼 만하다.


## 프로젝트 2: Youtube-search

### 문제 정의:

Youtube를 이용하는 과정에서, 최근 알고리즘 변경으로 인해 지나치게 검색 결과와 상관 없는 영상이 제시되기 시작했다. 이에 Youtube에서 제공하는 여러 서비스들, 즉 쇼츠 추천, 관심분야 관련 영상 추천, 최근 본 영상 관련 영상 추천 등, 검색 결과 확인에 방해되는 서비스들이 제외된 서비스를 구축하고자 했다.

### 기술 스택:

**Google-api-python-client:** 구글 서비스에 대한 RestAPI 접근을 python 환경에서 메서드 형태로 가능하게 해 주는 라이브러리이다.
**선택 이유:** 유튜브 검색 기능 구현을 위해서는 해당 라이브러리를 통한 쿼리가 직접 일일히 RestAPI로 구현하는 것 보다 낫다고 판단되었다.

### 처리 메커니즘:

**1.** app.js의 searchVideos가 입력된 쿼리문을 fetch 메서드를 통해 app.py로 전달한다

**2.** app.py가 request.args.get을 통해 쿼리문을 수신한다

**3.** 쿼리를 진행하기 전, googleapiclient.discovery.build를 통해 youtube API를 활성화

**4.** youtube.search.list().execute()를 통해 쿼리문에 대한 검색 결과 반환한다

**5.** video_data에 검색 정보들을 저장 및 json 형태로 app.js에 전달한다

**6.** app.js는 해당 json 값을 VideoCard 함수를 통해 최종 검색 결과들을 시각화 한다.

### 알고리즘:

기본적으로 검색 알고리즘 자체는 구글과 Youtube 자체 내장 검색/필터링 알고리즘에 의존하여 작동한다. 
그 외에는, youtube에 대한 쿼리 결과를 videoCard에 알맞게 원 반환문->딕셔너리->json 으로 변환하고, videoCard를 통해 웹에 동적으로 검색 결과를 표시한다.

### 개발 과정:

개발 도중, 검색한 내용을 videoCard 형태로 정리하여 웹에 띄우는 과정에서, 숙련도 부족으로 인해 여러 시행착오를 겪었다. 처음에는 Flask에서 query 결과를 가공할 때 어떤 식으로 딕셔너리에 접근해야 하는지 제대로 이해하지 못해 잘못된 값에 접근하는 일이 일어났었고, 그 다음에는 videoCard 형식 대신 모든 정보를 하나의 정적인 블록에서 일렬로 나열 하려다 구조적 한계에 봉착해, 지금의 videoCard 형태의 동적인 카드섹션 나열 구조로 최종 변경했다.

### 습득한 기술:

**구글 API 사용:** 구글 API 사용을 위해 구글 Cloud에서 직접 API key를 발급받고, 해당 도메인의 기능들을 사용해 보며 구글이 제공하는 기능들의 응용 경험을 쌓을 수 있었다.

**React에서 함수형 컴포넌트 사용:** Audio-Record와는 달리, videoCard와 같은 함수형 컴포넌트를 사용하여 웹에 동적 구성 요소를 도입하는 경험을 쌓을 수 있었다.

### 결론 및 개선 사항:

목표한 기능들이 정상 작동 하였기에 서비스 내적인 기능 문제는 없었다. 그러나 현실에서 검색을 단순히 키워드를 통해서만 하는 것이 아닌 만큼, 추가 개발 과정에서 필터링 등을 도입 시도 해 보는 것이 좋아 보인다.


## 프로젝트 3: Face-recog

### 문제 정의:

작년 컴퓨터 비전 수업을 들으며 여러가지 활동을 했었지만, 결국 실제로 작동하는 프로그램을 제작해 볼 기회는 없었다. 그렇기에 실전에서는 어떤 라이브러리가 사용되고, 어떤 식으로 프로그램이 구축되는지 체험해 보기 위해 본 프로그램을 구상 하게 되었다. 

### 기술 스택:

**Mediapipe:** Google에서 개발한 머신러닝 기반 화면 인식 라이브러리로, 이 프로젝트의 핵심인 얼굴 인식 기능을 포함하고 있다.
**선택 이유:** 직접 CNN 등을 구축하는 것 보다 압도적으로 쉽고 간편하고 가볍게 얼굴 인식 기능을 도입할 수 있었음.

**Opencv-python-headless:** Opencv의 그래픽 관련 기능 중, GUI 등 무겁고 경우에 따라 필요 없는 부분을 제거한 라이브러리이다. 주로 웹 환경이나 Docker, CLI 환경 등의 경우 GUI를 사용하지 않기에 해당 부분을 제거한 headless 버전을 사용하게 된다. 
**선택 이유:** 이 프로젝트의 개발 환경이 Docker 및 CLI 환경이었고, Flask를 이용한 웹 환경에서 구동되기 때문에 GUI 기능이 전혀 필요 없었다.

**Numpy:** Python에서 대규모 배열 및 행렬, 벡터 처리를 효율적으로 처리하고자 할 때 사용하는 라이브러리이다.
**선택 이유:** 유클리드 거리를 통한 유사도 분석에서 벡터 연산이 필요하기에 도입되었다..

### 처리 메커니즘:

**1.** index.html에 내장되어 있는 React 코드 중, FaceRecognitionApp의 registerFace가 작동하기 시작하면, captureImage()로 입력된 이미지 파일을 사용자 이름과 함께 app.py의 /register 루트로 전달한다

**2.** app.py가 해당 이미지를 수신하면, 우선 extract_face_features 함수를 통해 특징점을 추출한다. 이 과정에서 이미지를 cv2.cvtColor로 전처리를 하고 mediapipe의 여러 함수들, 즉 face_detection이나 face_mesh를 통해 얼굴을 배경으로부터 분리한 뒤 머신 러닝을 통해 특징점을 반환한다.

**3.** 추출된 특징점들을 데이터베이스 내 다른 얼굴들과 유클리드 거리 기반 비교를 거쳐 중복을 검사한다

**4.** 만약 중복이 아니라면, 해당 내용을 json의 형태로 저장한다

**5.** 이후 React의 recognizeFace가 작동하게 된다면, 이번에는 이미지 파일을 /recognize 루트로 전달한다

**6.** 이번에는 extract_face_features 결과를 단순히 데이터베이스 내 자료와 비교하면서 임의로 지정한 thresould보다 유사도가 높다면 해당 데이터와 비교 내용을 index.html에 전달한다

**7.** recognizeFace는 수신한 데이터를 drawBoundingBox로 넘겨 바운딩 박스를 그린 뒤, info-panel의 형태로 출력한다.

### 알고리즘:

Mediapipe 내에서 자체적으로 CNN(Convolutional Neural Network, 합성곱 신경망) 알고리즘을 통해 얼굴 추출 및 특징점 추출을 진행하며, 여기에 더해 웹 백엔드에서 데이터베이스 내의 정보와 Mediapipe를 통해 반환된 데이터 간의 유클리트 거리를 벡터연산을 통해 구해 유사도를 계산한다.

### 개발 과정:

여러 기능이 웹에 내장되는 만큼, 앞선 프로젝트들에 비해 웹 구조와 관련한 문제가 많았다. 최초 개발 당시 초기에는 얼굴 인식 데이터를 서버의 상태 변수에서 일시적으로 유지할 수 있을 것으로 판단했으나, 세션 간 데이터 유지가 불가능해 별도의 저장 구조가 필요함을 뒤늦게 인지 해, 수정했다..
오히려 우려했던 Mediapipe를 이용한 인공지능 인식 처리의 경우 본인이 이전에 했던 프로젝트에서 NLP를 다뤘던 경험과 작년에 이수한 컴퓨터 비전 수업에서의 지식 덕분에 다소 수월하게 진행할 수 있었다.

### 습득한 기술:

**mediapipe와 이를 위한 opencv 활용:** 도메인 최적화 경험이긴 하지만 mediapipe의 여러 기능을 경험해 볼 수 있었고, 또 opencv의 기능 또한 실제로 사용해 볼 수 있었다.

**복잡한 React/Flask 구조 경험:** Audio-Record와 비교했을 때, 좀 더 웹에서 많은 작업을 처리해야 했기에 그에 관련된 React와 Flask 구조 구축 및 활용 경험을 쌓을 수 있었다.

### 결론 및 개선 사항:

이 서비스를 테스트하는 과정에서 얼굴 각도, 표정 변화 등에 얼굴 인식이 민감하게 반응하는 모습을 보였다. 서비스 안정성을 위해서는 어느정도의 보정이 필요할 것으로 보인다.


## CI-CD 시스템:

### 문제 정의:

3개의 프로젝트를 하나의 github repository에 독립적으로 구축하면서, 기존의 CI-CD를 그대로 사용할 수 없다는 것을 확인하고, 새로 구축해야 했다.

### 기술 스택:

**Github Actions:** CI-CD workflow가 작동하기 위한 기반 플랫폼이다.
**선택 이유:** Github repository에 프로젝트를 업로드 하고, 이 repository 안에 CI-CD를 구축하는 데이는 Github Actions가 가장 편리한 방법이었다.

### 처리 메커니즘

**1.** push 주체 확인. main, develop, 그리고 feature/로 시작하는 브랜치에서 push가 발생하면 테스트 한다

**2.** pull request 대상 확인. main, develop에 대한 pull request는 병합된 상황을 가정하여 테스트 한다

**3.** jobs에 정의되어 있는 audio-record, youtube-search, face-recog 3개의 경우에 대해 각각 테스트를 진행한다. 단, youtube-search는 audio-record 이후, face-recog는 youtube-search에 대한 테스트 이후에 진행한다
테스트가 시작되면, -name과 run으로 정의된 절차를 따라 하나씩 테스트 해 나간다. 가장 먼저 requirement.txt에 기술되어 있는, 프로젝트가 의존하는 환경을 구축하게 하고, ngrok를 통해 로컬 환경에 접근 할 수 있게 한다.

**4.** 환경이 구축되었다면, 실제 코드를 실행해 각 jobs에서 요구하는 사항이 작동되는지 확인한다.

### 알고리즘:

Github Action에서 구현되는 CI-CD는 개별 job 내부적으로는 절차적으로, jobs 간에는 병렬적으로 처리된다. 그러나 현재 ngrok의 무료 서비스를 이용하는 현 상황에서는 jobs간 병렬 처리는 각 jobs 간 충돌을 유발한다. 이때 충돌이 발생한 다른 jobs는 해당 부분 검증이 건너 뛰어지기에, 결국 현재의 CI-CD는 병렬 처리가 아닌, needs 조건을 활용 해 jobs의 직렬 순차 처리를 하는 방식으로 구성되었다.

### 개발 과정:

기존에 사용하던 CI-CD 코드는 단일 job에 대한 코드였기에 이것을 여러 job으로 분할했다. 그러나 이 과정에서, github Actions가 job들을 병렬 처리 한다는 것을 간과했고, 이로 인해 3개의 job에서 첫번째 job에서만 ngrok 터널이 정상적으로 열리고, 나머지 둘에서는 ngrok 무료 서비스에서 터널을 오직 하나만 열게 해 주는 제약 조건으로 인해 테스트가 뛰어넘어 지는 문제가 발생했다.
이에 needs 조건을 도입해 각 job이 순차적으로 진행될 수 있도록 변경하여 문제를 해결하였다.

### 습득한 기술

**CI-CD 워크플로우를 yml 파일을 통해 정의:** 프로젝트 목표 달성을 위해 능동적으로 CI-CD 워크플로우를 변경하며, 성형하는 경험을 할 수 있었다.

### 결론 및 개선 사항:

pull request를 통한 CI-CD 검사는 성공적으로 수행되었다. 그러나 본인의 github branch 관리 및 commit 순서 실수 등으로 인해 push 검사는 큰 효과를 보지 못했으며, 3개의 프로젝트를 동시에 검사하는 구조로 인해 어느 한 프로젝트만 변경하는 경우 오버헤드가 발생하는 문제가 있었다. 추후 개발에서는 이런 요소들을 제거하는 것이 목표이다.
